% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SVD.R
\name{big_SVD}
\alias{big_SVD}
\title{SVD}
\usage{
big_SVD(X, fun.scaling, ind.row = seq(nrow(X)), block.size = 1000,
  k = NULL, thr.eigval = 1e-04)
}
\arguments{
\item{X}{A \link[=big.matrix-class]{big.matrix}.}

\item{fun.scaling}{A function that returns a named list of
\code{mean} and \code{sd} for every column, to scale each of their elements
such as followed: \deqn{\frac{X_{i,j} - mean_j}{sd_j}}.}

\item{ind.row}{An optional vector of the row indices that are used.
If not specified, all rows are used. \strong{Don't use negative indices.}}

\item{block.size}{Maximum number of columns read at once. Default is \code{1000}.}

\item{k}{Number of singular vectors/values to compute. Default is all.}

\item{thr.eigval}{Threshold to remove "unsignificant" singular vectors.
Default is \code{1e-4}.}
}
\value{
A list of
\itemize{
\item \code{d}, the singular values,
\item \code{u}, the left singular vectors,
\item \code{v}, the right singular vectors,
\item \code{means}, the centering vector,
\item \code{sds}, the scaling vector.
}

Note that to obtain the Principal Components, you must use
\code{big_predScoresPCA} on the result. See examples.
}
\description{
An algorithm for SVD (or PCA) of a \code{big.matrix} through the eigen
decomposition of the covariance between variables (primal)
or observations (dual).
}
\examples{
# Simulating some data
X <- big.matrix(73, 43)
X[] <- rnorm(length(X))

# Using only half of the data for "training"
ind <- sort(sample(nrow(X), nrow(X)/2))

test <- big_SVD(X = X,
                 fun.scaling = big_scale(),
                 ind.train = ind)
str(test)

pca <- prcomp(X[ind, ], center = TRUE, scale. = TRUE)

# same scaling
print(all.equal(test$means, pca$center))
print(all.equal(test$sds, pca$scale))

# scores and loadings are the same or opposite
# except for last eigenvalue which is equal to 0
# due to centering of columns
scores <- test$u \%*\% diag(test$d)
scores2 <- big_predScoresPCA(test) # use this function to predict scores
print(all.equal(scores, scores2))
print(dim(scores))
print(dim(pca$x))
print(tail(pca$sdev))
plot(scores2, pca$x[, 1:ncol(scores2)])
plot(test$v, pca$rotation[, 1:ncol(scores2)])

# projecting on new data
X.test <- sweep(sweep(X[-ind, ], 2, test$means, '-'), 2, test$sds, '/')
scores.test <- X.test \%*\% test$v
ind2 <- setdiff(seq(nrow(X)), ind)
scores.test2 <- big_predScoresPCA(test, X, ind.test = ind2) # use this
print(all.equal(scores.test, scores.test2))
scores.test3 <- predict(pca, X[-ind, ])
plot(scores.test2, scores.test3[, 1:ncol(scores.test2)])
################################################################################

## new scaling function
# "y-aware" scaling center columns, then multiply them by betas of
# univariate linear regression. See https://goo.gl/8G8WMa for details.
big_scaleYaware <- function(y) {
  function(X, ind.train = seq(nrow(X))) {
    means <- big_colstats(X, ind.train)$sum / length(ind.train)
    betas <- big_univLinReg(X, y, ind.train)$estim
    list(mean = means, sd = 1 / betas)
  }
}
# Simulating some data
y <- rnorm(nrow(X), X[, 9], abs(X[, 9]))

X.svd <- big_SVD(X, fun.scaling = big_scaleYaware(y))
plot(X.svd$v[, 1], type = "h")

################################################################################
}
\seealso{
\link[stats:prcomp]{prcomp}
}
