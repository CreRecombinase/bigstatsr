% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/randomSVD.R
\name{big_randomSVD}
\alias{big_randomSVD}
\title{Random SVD}
\usage{
big_randomSVD(X, fun.scaling, k = 10, tol = 1e-04, verbose = FALSE,
  ncores = 1)
}
\arguments{
\item{X}{A \link[bigmemory:big.matrix-class]{big.matrix}.
You shouldn't have missing values in your data.}

\item{fun.scaling}{A function that returns a named list of
\strong{\code{mean}} and \strong{\code{sd}} for every column, to scale each of their elements
such as followed: \deqn{\frac{X_{i,j} - mean_j}{sd_j}}.}

\item{k}{Number of PCs to compute. Default is \code{10}.}

\item{tol}{Precision parameter of \link[RSpectra:svds]{svds}.
Default is \code{1e-4}.}

\item{verbose}{Should some progress be print? Default is \code{FALSE}.}

\item{ncores}{Number or cores used. Default doesn't use parallelism.}
}
\value{
A list of
\itemize{
\item \code{d}, the singular values,
\item \code{u}, the left singular vectors if \code{returnU} is \code{TRUE},
\item \code{v}, the right singular vectors if \code{returnV} is \code{TRUE},
\item \code{niter}, the number of the iteration of the algorithm,
\item \code{nops}, number of Matrix-Vector multiplications used,
\item \code{means}, the centering vector,
\item \code{sds}, the scaling vector.
}

Note that to obtain the Principal Components, you must use
\code{big_predScoresPCA} on the result. See examples.
}
\description{
An algorithm for SVD (or PCA) of a \code{big.matrix} based on the algorithm
in RSpectra (by Yixuan Qiu and Jiali Mei).
This should be used to compute only a few PCs, but for very large matrices
because this algorithm is linear in all dimensions (in time) and is
very memory-efficient.
}
\examples{
# Simulating some data
X <- big.matrix(73, 43)
X[] <- rnorm(length(X))

# Using only half of the data for "training"
ind <- sort(sample(nrow(X), nrow(X)/2))

test <- big_SVD(X = X,
                 fun.scaling = big_scale(),
                 ind.train = ind)
str(test)

pca <- prcomp(X[ind, ], center = TRUE, scale. = TRUE)

# same scaling
print(all.equal(test$means, pca$center))
print(all.equal(test$sds, pca$scale))

# scores and loadings are the same or opposite
# except for last eigenvalue which is equal to 0
# due to centering of columns
scores <- test$u \%*\% diag(test$d)
scores2 <- big_predScoresPCA(test) # use this function to predict scores
print(all.equal(scores, scores2))
print(dim(scores))
print(dim(pca$x))
print(tail(pca$sdev))
plot(scores2, pca$x[, 1:ncol(scores2)])
plot(test$v, pca$rotation[, 1:ncol(scores2)])

# projecting on new data
X.test <- sweep(sweep(X[-ind, ], 2, test$means, '-'), 2, test$sds, '/')
scores.test <- X.test \%*\% test$v
ind2 <- setdiff(seq(nrow(X)), ind)
scores.test2 <- big_predScoresPCA(test, X, ind.test = ind2) # use this
print(all.equal(scores.test, scores.test2))
scores.test3 <- predict(pca, X[-ind, ])
plot(scores.test2, scores.test3[, 1:ncol(scores.test2)])

## new scaling function
# "y-aware" scaling center columns, then multiply them by betas of
# univariate linear regression. See https://goo.gl/8G8WMa for details.
big_scaleYaware <- function(y) {
  function(X, ind.train = seq(nrow(X))) {
    means <- big_colstats(X, ind.train)$sum / length(ind.train)
    betas <- big_univRegLin(X, y, ind.train)$estim
    list(mean = means, sd = 1 / betas)
  }
}
# Simulating some data
y <- rnorm(nrow(X), X[, 9], abs(X[, 9]))

X.svd <- big_SVD(X, fun.scaling = big_scaleYaware(y))
plot(X.svd$v[, 1], type = "h")
}
\seealso{
\link[RSpectra:svds]{svds} \link[flashpcaR:flashpca]{flashpca}
}
