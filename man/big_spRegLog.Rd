% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/biglasso.R
\name{big_spRegLog}
\alias{big_spRegLog}
\title{Sparse multivariate logistic regression}
\usage{
big_spRegLog(X, y01.train, ind.train = seq(nrow(X)), covar.train = NULL,
  ...)
}
\arguments{
\item{X}{A \link[bigmemory:big.matrix-class]{big.matrix}.
You shouldn't have missing values in your data.}

\item{y01.train}{Vector of responses, corresponding to \code{ind.train}.
Must be 0s and 1s.}

\item{ind.train}{An optional vector of the row indices that are used,
for the training part. If not specified, all rows are used.}

\item{covar.train}{Matrix of covariables to be added in each model to correct
for confounders (e.g. the scores of PCA), corresponding to \code{ind.train}.
Default is \code{NULL} and corresponds to only adding an Intercept to each model.}

\item{...}{Arguments passed on to \code{biglasso::biglasso}
\describe{
  \item{penalty}{The penalty to be applied to the model. Either "lasso" (the default), 
  "ridge", or "enet" (elastic net).}
  \item{ncores}{The number of OpenMP threads used for parallel computing.}
  \item{alpha}{The elastic-net mixing parameter that controls the relative contribution from the lasso (l1) and the ridge (l2) penalty. The penalty is defined as \deqn{ \alpha||\beta||_1  + (1-\alpha)/2||\beta||_2^2.} \code{alpha=1} is the lasso penalty, \code{alpha=0} the ridge penalty, \code{alpha} in between 0 and 1 is the elastic-net ("enet") penalty. }
  \item{lambda.min}{The smallest value for lambda, as a fraction of
    lambda.max.  Default is .001 if the number of observations is larger
    than the number of covariates and .05 otherwise.}
  \item{nlambda}{The number of lambda values.  Default is 100.}
  \item{dfmax}{Upper bound for the number of nonzero coefficients.
    Default is no upper bound.  However, for large data sets,
    computational burden may be heavy for models with a large number of
    nonzero coefficients.}
  \item{verbose}{Whether to output the timing of each lambda iteration. Default is FALSE.}
}}
}
\value{
An object with S3 class \code{"biglasso"} with following variables.
  \item{beta}{The fitted matrix of coefficients, store in sparse matrix representation. The number of rows is equal to the number of coefficients, whereas the number of columns is equal to \code{nlambda}.}
  \item{iter}{A vector of length \code{nlambda} containing the number of iterations until convergence at each value of \code{lambda}.}
  \item{lambda}{The sequence of regularization parameter values in the path.}
  \item{penalty}{Same as above.}
  \item{family}{Same as above.}
  \item{alpha}{Same as above.}
  \item{loss}{A vector containing either the residual sum of squares (\code{for "gaussian"}) or negative log-likelihood (for \code{"binomial"}) of the fitted model at each value of \code{lambda}.}
  \item{penalty.factor}{Same as above.}
  \item{n}{The number of observations used in the model fitting. It's equal to \code{length(row.idx)}.}
  \item{center}{The sample mean vector of the variables, i.e., column mean of the sub-matrix of \code{X} used for model fitting.}
  \item{scale}{The sample standard deviation of the variables, i.e., column standard deviation of the sub-matrix of \code{X} used for model fitting.}
  \item{y}{The response vector used in the model fitting. Depending on \code{row.idx}, it could be a subset of the raw input of the response vector y.}
  \item{screen}{Same as above.}
  \item{col.idx}{The indices of features that have 'scale' value greater than 1e-6. Features with 'scale' less than 1e-6 are removed from model fitting.}
  \item{rejections}{The number of features rejected at each value of \code{lambda}.}
  \item{safe_rejections}{The number of features rejected by safe rules at each value of \code{lambda}. Only for "SSR-Dome", "SSR-BEDPP" and "SSR-Slores" cases.}
}
\description{
Fit lasso penalized logistic regression path for a \code{big.matrix}.
Covariates can be added to correct for confounders.
This is a wrapper of \link[biglasso:biglasso]{biglasso}.
}
\examples{

}
\references{
Tibshirani, R., Bien, J., Friedman, J., Hastie, T.,
Simon, N., Taylor, J. and Tibshirani, R. J. (2012),
Strong rules for discarding predictors in lasso-type problems.
Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 74: 245â€“266.
\url{http://dx.doi.org/10.1111/j.1467-9868.2011.01004.x}.
}
\seealso{
\link[glmnet:glmnet]{glmnet} \link[biglasso:biglasso]{biglasso}
}
