% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sparseSVM.R
\name{big_spSVM}
\alias{big_spSVM}
\title{Sparse SVM}
\usage{
big_spSVM(X, y01.train, ind.train = seq(nrow(X)), covar.train = NULL, ...)
}
\arguments{
\item{X}{A \link[bigmemory:big.matrix-class]{big.matrix}.
You shouldn't have missing values in your data.}

\item{y01.train}{Vector of responses, corresponding to \code{ind.train}.
Must be 0s and 1s.}

\item{ind.train}{An optional vector of the row indices that are used,
for the training part. If not specified, all rows are used.}

\item{covar.train}{Matrix of covariables to be added in each model to correct
for confounders (e.g. the scores of PCA), corresponding to \code{ind.train}.
Default is \code{NULL} and corresponds to only adding an Intercept to each model.}

\item{...}{Arguments passed on to \code{sparseSVM::sparseSVM}
\describe{
  \item{alpha}{The elastic-net mixing parameter that controls the relative contribution 
    from the lasso and the ridge penalty. It must be a number between 0 and 1. \code{alpha=1} 
    is the lasso penalty and \code{alpha=0} the ridge penalty.}
  \item{gamma}{The tuning parameter for huberization smoothing of hinge loss. Default is 0.1.}
  \item{nlambda}{The number of lambda values.  Default is 100.}
  \item{lambda.min}{The smallest value for lambda, as a fraction of lambda.max, the data 
    derived entry value. Default is 0.01 if the number of observations is larger than the 
    number of variables and 0.05 otherwise.}
  \item{screen}{Screening rule to be applied at each \code{lambda} that discards variables 
    for speed. Either "ASR" (default), "SR" or "none". "SR" stands for the strong rule, 
    and "ASR" for the adaptive strong rule. Using "ASR" typically requires fewer iterations 
    to converge than "SR", but the computing time are generally close. Note that the option 
    "none" is used mainly for debugging, which may lead to much longer computing time.}
  \item{dfmax}{Upper bound for the number of nonzero coefficients. The algorithm exits and 
    returns a partial path if \code{dfmax} is reached. Useful for very large dimensions.}
  \item{message}{If set to TRUE,  sparseSVM will inform the user of its progress. This argument 
    is kept for debugging. Default is FALSE.}
}}
}
\value{
The function returns an object of S3 class \code{"sparseSVM"}, which is a list containing:
  \item{call}{The call that produced this object.}
  \item{weights}{The fitted matrix of coefficients.  The number of rows is equal to the number 
    of coefficients, and the number of columns is equal to \code{nlambda}. An intercept is included.}
  \item{iter}{A vector of length \code{nlambda} containing the number of iterations until 
    convergence at each value of \code{lambda}.}
  \item{saturated}{A logical flag for whether the number of nonzero coefficients has reached \code{dfmax}.}
  \item{lambda}{The sequence of regularization parameter values in the path.}
  \item{alpha}{Same as above.}
  \item{gamma}{Same as above.}
  \item{penalty.factor}{Same as above.}
  \item{levels}{Levels of the output class labels.}
}
\description{
Fit solution paths for sparse linear SVM regularized by lasso or elastic-net
over a grid of values for the regularization parameter lambda.
This is a wrapper of a modified version of
\link[sparseSVM:sparseSVM]{sparseSVM}.
}
\examples{

}
\seealso{
\link[LiblineaR:LiblineaR]{LiblineaR} \link[sparseSVM:sparseSVM]{sparseSVM}
}
